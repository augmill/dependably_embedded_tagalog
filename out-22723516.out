========================================
Starting Contrastive Learning Training (CL-Only Ablation)
Training base BERT with CL - NO stage 1 MLM finetuning
========================================
Job ID: 22723516
Node: bgpu-g4-u32
Start time: Sun Feb  8 11:09:06 MST 2026

/var/spool/slurmd/job22723516/slurm_script: line 42: /curc/sw/anaconda3/latest/etc/profile.d/conda.sh: Not a directory
Environment activated:
  Python: /projects/pabo8622/software/anaconda/envs/cbert/bin/python
  Conda env: cbert

GPU Information:
Sun Feb  8 11:09:07 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:21:00.0 Off |                   On |
| N/A   51C    P0            169W /  350W |    4746MiB /  81559MiB |     N/A      Default |
|                                         |                        |              Enabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:81:00.0 Off |                   On |
| N/A   26C    P0             43W /  350W |       1MiB /  81559MiB |     N/A      Default |
|                                         |                        |              Enabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:E2:00.0 Off |                   On |
| N/A   27C    P0             43W /  350W |     101MiB /  81559MiB |     N/A      Default |
|                                         |                        |              Enabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| MIG devices:                                                                            |
+------------------+----------------------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|        Shared         |
|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC  DEC  OFA  JPG |
|                  |                                  |        ECC|                       |
|==================+==================================+===========+=======================|
|  0    3   0   0  |              29MiB / 20096MiB    | 30      0 |  2   0    2    0    2 |
|                  |                 0MiB / 32767MiB  |           |                       |
+------------------+----------------------------------+-----------+-----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

PyTorch: 2.5.1+cu121
CUDA available: True

Training Configuration:
  Data file: /projects/pabo8622/dependably_embedded_tagalog/data/checked_graphs.jsonl
  BERT model: google-bert/bert-base-multilingual-cased (base BERT, no stage 1 MLM)
  Output dir: /projects/pabo8622/dependably_embedded_tagalog/stage_2_finetuning/cl_model_base_only
  Epochs: 50
  Batch size: 24
  Learning rate: 3e-5
  Temperature: 0.07
  Projection dim: 256
  Warmup steps: 300

Note: This is the CL-only ablation (no stage 1 MLM finetuning)
Note: BERT embeddings will be fine-tuned (not frozen)

Starting training...
========================================
Traceback (most recent call last):
  File "/projects/pabo8622/dependably_embedded_tagalog/stage_2_finetuning/train_cl.py", line 557, in <module>
    main()
  File "/projects/pabo8622/dependably_embedded_tagalog/stage_2_finetuning/train_cl.py", line 443, in main
    wandb.init(
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1603, in init
    get_sentry().reraise(e)
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/analytics/sentry.py", line 190, in reraise
    raise exc.with_traceback(tb)
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1524, in init
    wi.maybe_login(init_settings)
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 183, in maybe_login
    run_settings = self._wl.settings.model_copy()
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py", line 307, in settings
    self._load_settings(
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py", line 243, in _load_settings
    self._settings.update_from_env_vars(self._settings_environ)
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/sdk/wandb_settings.py", line 1911, in update_from_env_vars
    setattr(self, key, value)
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/pydantic/main.py", line 1033, in __setattr__
    setattr_handler(self, name, value)  # call here to not memo on possibly unknown fields
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/pydantic/main.py", line 111, in <lambda>
    'validate_assignment': lambda model, name, val: model.__pydantic_validator__.validate_assignment(model, name, val),  # pyright: ignore[reportAssignmentType]
  File "/projects/pabo8622/software/anaconda/envs/cbert/lib/python3.10/site-packages/wandb/sdk/wandb_settings.py", line 1074, in validate_api_key
    raise UsageError("API key cannot start or end with whitespace")
wandb.errors.errors.UsageError: API key cannot start or end with whitespace

========================================
Job completed with exit code: 1
End time: Sun Feb  8 11:10:33 MST 2026
========================================
Training failed with exit code 1
